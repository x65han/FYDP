{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Train and Test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1622-uPPLKJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viMz-yOLLRDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.contrib import keras\n",
        "import re\n",
        "L = keras.layers\n",
        "K = keras.backend\n",
        "import tensorflow as tf\n",
        "import threading\n",
        "import json\n",
        "import google.colab as colab\n",
        "import cv2\n",
        "import zipfile\n",
        "import pickle\n",
        "import collections\n",
        "import random\n",
        "\n",
        "\n",
        "from utils.file_utils import *\n",
        "from utils.image_utils import *\n",
        "from utils.generator_utils import *\n",
        "from utils.tqdm_utils import *\n",
        "from utils.keras_utils import *\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NG820XXRp5C",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JQTQ3LtRe9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mount_google_drive():\n",
        "\t'''\n",
        "\t# Functionality\n",
        "\t\tMount google drive. Since colab does not save files, we want to make it easier to directly access files in google drive.\n",
        "\t# Arguments\n",
        "\t\tNothing\n",
        "\t# Returns\n",
        "\t\tdrive_root: the working directory mounted\n",
        "\t'''\n",
        "\tmount_directory = \"/content/gdrive\"\n",
        "\tdrive = colab.drive\n",
        "\tdrive.mount(mount_directory, force_remount=True)\n",
        "\tdrive_root = mount_directory + \"/\" + list(filter(lambda x: x[0] != '.', os.listdir(mount_directory)))[0]\n",
        "\treturn drive_root"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOXLk--fVkAt",
        "colab_type": "code",
        "outputId": "9165653a-d00c-4a96-970e-b0606b65d6d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ROOT_DIR =  mount_google_drive()\n",
        "CHECKPOINT_ROOT = ROOT_DIR+ \"/captioning/checkpoints/\"\n",
        "DATASET_DIR = ROOT_DIR + \"/Dataset/\"\n",
        "\n",
        "if not os.path.exists(DATASET_DIR):\n",
        "  os.makedirs(DATASET_DIR)\n",
        "\n",
        "if not os.path.exists(CHECKPOINT_ROOT):\n",
        "  os.makedirs(CHECKPOINT_ROOT)\n",
        "\n",
        "def get_checkpoint_path(epoch=None):\n",
        "    if epoch is None:\n",
        "        return os.path.abspath(CHECKPOINT_ROOT + \"weights\")\n",
        "    else:\n",
        "        return os.path.abspath(CHECKPOINT_ROOT + \"weights_{}\".format(epoch))\n",
        "      \n",
        "# example of checkpoint dir\n",
        "print(get_checkpoint_path(4))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/captioning/checkpoints/weights_4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tkdGObeNni2",
        "colab_type": "text"
      },
      "source": [
        "## Load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYpMgAd9N4cV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_pickle(obj, fn):\n",
        "\t'''\n",
        "\t# Functionality\n",
        "\t\tSave the data into pickle format\n",
        "\t# Arguments\n",
        "\t\tobj: the data object\n",
        "\t\tfn: the pickle file name\n",
        "\t# Returns\n",
        "\t\tNothing. Just save to the file.\n",
        "\t'''\n",
        "\twith open(fn, \"wb\") as f:\n",
        "\t\tpickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def read_pickle(fn):\n",
        "\n",
        "\t'''\n",
        "\t# Functionality\n",
        "\t\tSave the data into pickle format\n",
        "\t# Arguments\n",
        "\t\tfn: the pickle file name\n",
        "\t# Returns\n",
        "\t\tobj: the desired data object\n",
        "\t'''\n",
        "\twith open(fn, \"rb\") as f:\n",
        "\t\treturn pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg3rjRWwNqdZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3c6ed947-4678-4bac-b674-32aba7b75605"
      },
      "source": [
        "train_img_embeds = read_pickle(DATASET_DIR + \"train_img_embeds.pickle\")\n",
        "train_img_fns = read_pickle(DATASET_DIR + \"train_img_fns.pickle\")\n",
        "val_img_embeds = read_pickle(DATASET_DIR + \"val_img_embeds.pickle\")\n",
        "val_img_fns = read_pickle(DATASET_DIR + \"val_img_fns.pickle\")\n",
        "# check shapes\n",
        "print(train_img_embeds.shape, len(train_img_fns))\n",
        "print(val_img_embeds.shape, len(val_img_fns))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8278, 2048) 8278\n",
            "(4050, 2048) 4050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ_72D0GOcrn",
        "colab_type": "text"
      },
      "source": [
        "## Extract Labels (Captions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_JL7coYNyuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract captions from zip\n",
        "def get_captions_for_fns(fns, zip_fn, zip_json_path):\n",
        "    zf = zipfile.ZipFile(zip_fn)\n",
        "    j = json.loads(zf.read(zip_json_path).decode(\"utf8\"))\n",
        "    id_to_fn = {img[\"id\"]: img[\"file_name\"] for img in j[\"images\"]}\n",
        "    fn_to_caps = defaultdict(list)\n",
        "    for cap in j['annotations']:\n",
        "        fn_to_caps[id_to_fn[cap['image_id']]].append(cap['caption'])\n",
        "    fn_to_caps = dict(fn_to_caps)\n",
        "    return list(map(lambda x: fn_to_caps[x], fns))\n",
        "    \n",
        "train_captions = get_captions_for_fns(train_img_fns, DATASET_DIR + \"captions_train-val2014.zip\", \n",
        "                                      \"annotations/captions_train2014.json\")\n",
        "\n",
        "val_captions = get_captions_for_fns(val_img_fns, DATASET_DIR + \"captions_train-val2014.zip\", \n",
        "                                      \"annotations/captions_val2014.json\")\n",
        "\n",
        "# check shape\n",
        "assert len(train_img_fns) == len(train_captions)\n",
        "assert len(val_img_fns) == len(val_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYQ3p8uDOjvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1e6019b2-1462-4a61-c370-2aaa1a83398b"
      },
      "source": [
        "train_captions[1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a man with a beard and a brown and white dog on a leash',\n",
              " 'A man and his dog on a hike - both wearing backpacks',\n",
              " 'A man and a dog standing on a dirt path in the woods.',\n",
              " 'a man carrying a back pack walking a dog carrying a back pack',\n",
              " 'A man with a backpack holding a dog on a leash.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwbwEzO4REuG",
        "colab_type": "text"
      },
      "source": [
        "## Let's go to NLP Part now!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpE30yhUSdr8",
        "colab_type": "text"
      },
      "source": [
        "### General Pipeline for captions of each image:\n",
        "- Original Captions is a list of sentences of various lengths\n",
        "-  Change every word to lower case\n",
        "- Tokenize the senteces and build vocabulary (with specific threashold word frequencies). \n",
        "- Map each token to a number.\n",
        "- Sentences then become list of list of numbers\n",
        "- Pad and truncate the sequence of numbers to max len"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCAHm5GuQynJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# special tokens\n",
        "PAD = \"#PAD#\"\n",
        "UNK = \"#UNK#\"\n",
        "START = \"#START#\"\n",
        "END = \"#END#\"\n",
        "\n",
        "# split sentence into tokens (split into lowercased words)\n",
        "def split_sentence(sentence):\n",
        "    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n",
        "\n",
        "  \n",
        "def generate_vocabulary(train_captions, freq_threshold=3):\n",
        "    \"\"\"\n",
        "    Return {token: index} for all train tokens (words) that occur 5 times or more, \n",
        "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
        "    Use `split_sentence` function to split sentence into tokens.\n",
        "    Also, add PAD (for batch padding), UNK (unknown, out of vocabulary), \n",
        "        START (start of sentence) and END (end of sentence) tokens into the vocabulary.\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_with_repeat = []\n",
        "    for captions in train_captions:\n",
        "      for sentence in captions:\n",
        "        tokens = split_sentence(sentence)\n",
        "        vocab_with_repeat += tokens\n",
        "    counter = collections.Counter(vocab_with_repeat)\n",
        "    \n",
        "    vocab = []\n",
        "    for element in counter:\n",
        "      if counter[element] >= freq_threshold:\n",
        "        vocab.append(element)\n",
        "    vocab = list(set(vocab))\n",
        "    vocab += [PAD, UNK, START, END]\n",
        "    \n",
        "    return {token: index for index, token in enumerate(sorted(vocab))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVbH5C_SVGZk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d7d6755-ab87-4caa-c7db-4251ef1ba3a0"
      },
      "source": [
        "# prepare vocabulary\n",
        "vocab = generate_vocabulary(train_captions)\n",
        "vocab_inverse = {idx: w for w, idx in vocab.items()}\n",
        "print(len(vocab))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhobnh3mRb5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caption_tokens_to_indices(captions, vocab):\n",
        "    \"\"\"\n",
        "    `captions` argument is an array of arrays:\n",
        "    [\n",
        "        [\n",
        "            \"image1 caption1\",\n",
        "            \"image1 caption2\",\n",
        "            ...\n",
        "        ],\n",
        "        [\n",
        "            \"image2 caption1\",\n",
        "            \"image2 caption2\",\n",
        "            ...\n",
        "        ],\n",
        "        ...\n",
        "    ]\n",
        "    Use `split_sentence` function to split sentence into tokens.\n",
        "    Replace all tokens with vocabulary indices, use UNK for unknown words (out of vocabulary).\n",
        "    Add START and END tokens to start and end of each sentence respectively.\n",
        "    For the example above you should produce the following:\n",
        "    [\n",
        "        [\n",
        "            [vocab[START], vocab[\"caption1\"],...,vocab[END]],\n",
        "            [vocab[START], vocab[\"caption2\"],...,vocab[END]],\n",
        "            ...\n",
        "        ],\n",
        "        ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    for instance in captions:\n",
        "      instance_list = []\n",
        "      \n",
        "      for sentence in instance:\n",
        "        sentence_list = []\n",
        "        sentence_list.append(vocab[START])\n",
        "        \n",
        "        # append tokens\n",
        "        tokens = split_sentence(sentence)\n",
        "        for token in tokens:\n",
        "          if token not in vocab.keys():\n",
        "            sentence_list.append(vocab[UNK])\n",
        "          else:\n",
        "            sentence_list.append(vocab[token])\n",
        "        \n",
        "        sentence_list.append(vocab[END])\n",
        "        \n",
        "        instance_list.append(sentence_list)\n",
        "      res.append(instance_list)\n",
        "\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVuE-JrpRe5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will use this during training\n",
        "def batch_captions_to_matrix(batch_captions, pad_idx, max_len=None):\n",
        "    \"\"\"\n",
        "    `batch_captions` is an array of arrays:\n",
        "    [\n",
        "        [vocab[START], ..., vocab[END]],\n",
        "        [vocab[START], ..., vocab[END]],\n",
        "        ...\n",
        "    ]\n",
        "    Put vocabulary indexed captions into np.array of shape (len(batch_captions), columns),\n",
        "        where \"columns\" is max(map(len, batch_captions)) when max_len is None\n",
        "        and \"columns\" = min(max_len, max(map(len, batch_captions))) otherwise.\n",
        "    Add padding with pad_idx where necessary.\n",
        "    Input example: [[1, 2, 3], [4, 5]]\n",
        "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=None\n",
        "    Output example: np.array([[1, 2], [4, 5]]) if max_len=2\n",
        "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=100\n",
        "    Try to use numpy, we need this function to be fast!\n",
        "    \"\"\"\n",
        "    matrix = []\n",
        "    \n",
        "    if max_len:\n",
        "      max_len = min(max_len, len(max(batch_captions, key=len)))  \n",
        "    else:\n",
        "      max_len = len(max(batch_captions, key=len))\n",
        "    \n",
        "    for caption in batch_captions:\n",
        "      if len(caption) < max_len:\n",
        "        output = caption + [pad_idx] * (max_len-len(caption))\n",
        "      elif len(caption) > max_len:\n",
        "        output = caption[:max_len]\n",
        "      else:\n",
        "        output = caption\n",
        "      matrix.append(output)\n",
        "\n",
        "    return np.array(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvulFGhgU5bS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d61fbdb-f49f-4eaf-d568-1dce07075f0c"
      },
      "source": [
        "# make sure you use correct argument in caption_tokens_to_indices\n",
        "assert len(caption_tokens_to_indices(train_captions[:10], vocab)) == 10\n",
        "assert len(caption_tokens_to_indices(train_captions[:5], vocab)) == 5\n",
        "caption_tokens_to_indices(train_captions[:3], vocab)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[2, 16, 4075, 2685, 4111, 16, 452, 99, 4075, 1429, 0],\n",
              "  [2, 16, 2631, 2382, 681, 528, 2396, 3778, 2382, 16, 4075, 2685, 0],\n",
              "  [2, 16, 681, 528, 99, 16, 1393, 2896, 3747, 270, 1144, 0],\n",
              "  [2, 16, 681, 993, 2396, 16, 2685, 4111, 16, 1393, 0],\n",
              "  [2,\n",
              "   16,\n",
              "   2631,\n",
              "   2382,\n",
              "   681,\n",
              "   1003,\n",
              "   2396,\n",
              "   16,\n",
              "   2685,\n",
              "   4111,\n",
              "   16,\n",
              "   2300,\n",
              "   99,\n",
              "   16,\n",
              "   1393,\n",
              "   0]],\n",
              " [[2, 16, 2108, 4111, 16, 279, 99, 16, 451, 99, 4075, 1057, 2396, 16, 1980, 0],\n",
              "  [2, 16, 2108, 99, 1702, 1057, 2396, 16, 3, 405, 4047, 195, 0],\n",
              "  [2, 16, 2108, 99, 16, 1057, 3452, 2396, 16, 1031, 2537, 1784, 3691, 4122, 0],\n",
              "  [2, 16, 2108, 591, 16, 190, 2455, 4001, 16, 1057, 591, 16, 190, 2455, 0],\n",
              "  [2, 16, 2108, 4111, 16, 194, 1712, 16, 1057, 2396, 16, 1980, 0]],\n",
              " [[2,\n",
              "   16,\n",
              "   189,\n",
              "   1784,\n",
              "   16,\n",
              "   2907,\n",
              "   3539,\n",
              "   3188,\n",
              "   154,\n",
              "   2396,\n",
              "   1702,\n",
              "   2241,\n",
              "   3042,\n",
              "   667,\n",
              "   0],\n",
              "  [2, 16, 2040, 189, 3689, 1834, 3297, 2396, 3373, 0],\n",
              "  [2,\n",
              "   16,\n",
              "   4151,\n",
              "   421,\n",
              "   1784,\n",
              "   16,\n",
              "   2907,\n",
              "   99,\n",
              "   4075,\n",
              "   3541,\n",
              "   3188,\n",
              "   2338,\n",
              "   3747,\n",
              "   1702,\n",
              "   3,\n",
              "   434,\n",
              "   0],\n",
              "  [2, 16, 189, 1784, 16, 3603, 2303, 2396, 16, 4116, 3042, 667, 0],\n",
              "  [2, 95, 1781, 2382, 189, 3297, 2338, 3747, 16, 4116, 0]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt47Vudnjjv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace tokens with indices\n",
        "train_captions_indexed = caption_tokens_to_indices(train_captions, vocab)\n",
        "val_captions_indexed = caption_tokens_to_indices(val_captions, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0efRCKQX17K",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlog_CIrXJ51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_EMBED_SIZE = train_img_embeds.shape[1]\n",
        "IMG_EMBED_BOTTLENECK = 120\n",
        "WORD_EMBED_SIZE = 100\n",
        "LSTM_UNITS = 300\n",
        "LOGIT_BOTTLENECK = 120\n",
        "pad_idx = vocab[PAD]\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 12\n",
        "n_batches_per_epoch = 1000\n",
        "n_validation_batches = 100  # how many batches are used for validation after each epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAtaH-RzX3xL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remember to reset your graph if you want to start building it from scratch!\n",
        "s = reset_tf_session()\n",
        "tf.set_random_seed(46)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74_P-AYkenCK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "62623dbd-45a1-44f3-c868-5b0934b73de7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul 11 01:00:14 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    29W /  70W |    129MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbSv8Ow8auu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        },
        "outputId": "b019c295-3732-4697-ef4e-e84ec9b68449"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "Image(url= \"https://camo.githubusercontent.com/040ce95dff6d10fd3ae4c47c4a3e01ab6abed220/68747470733a2f2f6769746875622e636f6d2f6873652d616d6c2f696e74726f2d746f2d646c2f626c6f622f6d61737465722f7765656b362f696d616765732f666c617474656e5f68656c702e6a70673f7261773d31\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://camo.githubusercontent.com/040ce95dff6d10fd3ae4c47c4a3e01ab6abed220/68747470733a2f2f6769746875622e636f6d2f6873652d616d6c2f696e74726f2d746f2d646c2f626c6f622f6d61737465722f7765656b362f696d616765732f666c617474656e5f68656c702e6a70673f7261773d31\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWb-5WGVX6I1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class decoder:\n",
        "    # [batch_size, IMG_EMBED_SIZE] of CNN image features\n",
        "    img_embeds = tf.placeholder('float32', [None, IMG_EMBED_SIZE])\n",
        "    # [batch_size, time steps] of word ids\n",
        "    sentences = tf.placeholder('int32', [None, None])\n",
        "    \n",
        "    # we use bottleneck here to reduce the number of parameters\n",
        "    # image embedding -> bottleneck\n",
        "    img_embed_to_bottleneck = L.Dense(IMG_EMBED_BOTTLENECK, \n",
        "                                      input_shape=(None, IMG_EMBED_SIZE), \n",
        "                                      activation='elu')\n",
        "    # image embedding bottleneck -> lstm initial state\n",
        "    img_embed_bottleneck_to_h0 = L.Dense(LSTM_UNITS,\n",
        "                                         input_shape=(None, IMG_EMBED_BOTTLENECK),\n",
        "                                         activation='elu')\n",
        "    # word -> embedding\n",
        "    word_embed = L.Embedding(len(vocab), WORD_EMBED_SIZE)\n",
        "    # lstm cell (from tensorflow)\n",
        "    lstm = tf.nn.rnn_cell.LSTMCell(LSTM_UNITS)\n",
        "    \n",
        "    # we use bottleneck here to reduce model complexity\n",
        "    # lstm output -> logits bottleneck\n",
        "    token_logits_bottleneck = L.Dense(LOGIT_BOTTLENECK, \n",
        "                                      input_shape=(None, LSTM_UNITS),\n",
        "                                      activation=\"elu\")\n",
        "    # logits bottleneck -> logits for next token prediction\n",
        "    token_logits = L.Dense(len(vocab),\n",
        "                           input_shape=(None, LOGIT_BOTTLENECK))\n",
        "    \n",
        "    # initial lstm cell state of shape (None, LSTM_UNITS),\n",
        "    # we need to condition it on `img_embeds` placeholder.\n",
        "    \n",
        "    c0 = h0 = img_embed_bottleneck_to_h0(img_embed_to_bottleneck(img_embeds))\n",
        "\n",
        "    # embed all tokens but the last (last for not be input) for lstm input,\n",
        "    # remember that L.Embedding is callable,\n",
        "    # use `sentences` placeholder as input.\n",
        "    \n",
        "    word_embeds = word_embed(sentences[:, :-1])\n",
        "    \n",
        "    # during training we use ground truth tokens `word_embeds` as context for next token prediction.\n",
        "    # that means that we know all the inputs for our lstm and can get \n",
        "    # all the hidden states with one tensorflow operation (tf.nn.dynamic_rnn).\n",
        "    # `hidden_states` has a shape of [batch_size, time steps, LSTM_UNITS].\n",
        "    hidden_states, _ = tf.nn.dynamic_rnn(lstm, word_embeds,\n",
        "                                         initial_state=tf.nn.rnn_cell.LSTMStateTuple(c0, h0))\n",
        "\n",
        "    # now we need to calculate token logits for all the hidden states\n",
        "    \n",
        "    # first, we reshape `hidden_states` to [-1, LSTM_UNITS]\n",
        "    \n",
        "    flat_hidden_states = tf.reshape(hidden_states, [-1, LSTM_UNITS])\n",
        "\n",
        "    # then, we calculate logits for next tokens using `token_logits_bottleneck` and `token_logits` layers\n",
        "    \n",
        "    flat_token_logits = token_logits(token_logits_bottleneck(flat_hidden_states))\n",
        "    \n",
        "    # then, we flatten the ground truth (output side, compare with word_embeds) token ids.\n",
        "    # remember, that we predict next tokens for each time step,\n",
        "    # use `sentences` placeholder.\n",
        "    \n",
        "    flat_ground_truth = tf.reshape(sentences[:, 1:], [-1])\n",
        "\n",
        "    # we need to know where we have real tokens (not padding) in `flat_ground_truth`,\n",
        "    # we don't want to propagate the loss for padded output tokens,\n",
        "    # fill `flat_loss_mask` with 1.0 for real tokens (not pad_idx) and 0.0 otherwise.\n",
        "    \n",
        "    flat_loss_mask = tf.cast(tf.not_equal(flat_ground_truth, pad_idx), 'float32')\n",
        "\n",
        "    # compute cross-entropy between `flat_ground_truth` and `flat_token_logits` predicted by lstm\n",
        "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=flat_ground_truth, \n",
        "        logits=flat_token_logits\n",
        "    )\n",
        "\n",
        "    # compute average `xent` over tokens with nonzero `flat_loss_mask`.\n",
        "    # we don't want to account misclassification of PAD tokens, because that doesn't make sense,\n",
        "    # we have PAD tokens for batching purposes only!\n",
        "    \n",
        "    loss = tf.reduce_sum(xent*flat_loss_mask) / tf.reduce_sum(flat_loss_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0v9SCZPZrG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(images_embeddings, indexed_captions, batch_size, max_len=None):\n",
        "    \"\"\"\n",
        "    `images_embeddings` is a np.array of shape [number of images, IMG_EMBED_SIZE].\n",
        "    `indexed_captions` holds 5 vocabulary indexed captions for each image:\n",
        "    [\n",
        "        [\n",
        "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
        "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
        "            ...\n",
        "        ],\n",
        "        ...\n",
        "    ]\n",
        "    Generate a random batch of size `batch_size`.\n",
        "    Take random images and choose one random caption for each image.\n",
        "    Remember to use `batch_captions_to_matrix` for padding and respect `max_len` parameter.\n",
        "    Return feed dict {decoder.img_embeds: ..., decoder.sentences: ...}.\n",
        "    \"\"\"\n",
        "    indices = np.random.randint(0, len(images_embeddings), batch_size)\n",
        "    batch_image_embeddings = images_embeddings[indices]\n",
        "    \n",
        "    batch_captions = []\n",
        "    for i in indices:\n",
        "      all_current_captions = indexed_captions[i]\n",
        "      cap_idx = np.random.randint(0, len(all_current_captions))\n",
        "      batch_captions.append(all_current_captions[cap_idx])\n",
        "    batch_captions_matrix = batch_captions_to_matrix(batch_captions, pad_idx, max_len)\n",
        "    \n",
        "    return {decoder.img_embeds: batch_image_embeddings, \n",
        "            decoder.sentences: batch_captions_matrix}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8EIPalvjcjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define optimizer operation to minimize the loss\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "train_step = optimizer.minimize(decoder.loss)\n",
        "\n",
        "# will be used to save/load network weights.\n",
        "# you need to reset your default graph and define it in the same way to be able to load the saved weights!\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# intialize all variables\n",
        "s.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKo1SKLVjBJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "d9790184-17a5-446a-9aec-3935629253c9"
      },
      "source": [
        "# actual training loop\n",
        "MAX_LEN = 20  # truncate long captions to speed up training\n",
        "\n",
        "np.random.seed(46)\n",
        "random.seed(46)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    pbar = tqdm_notebook_failsafe(range(n_batches_per_epoch))\n",
        "    counter = 0\n",
        "    for _ in pbar:\n",
        "        train_loss += s.run([decoder.loss, train_step], \n",
        "                            generate_batch(train_img_embeds, \n",
        "                                           train_captions_indexed, \n",
        "                                           batch_size, \n",
        "                                           MAX_LEN))[0]\n",
        "        counter += 1\n",
        "        pbar.set_description(\"Training loss: %f\" % (train_loss / counter))\n",
        "        \n",
        "    train_loss /= n_batches_per_epoch\n",
        "    \n",
        "    val_loss = 0\n",
        "    for _ in range(n_validation_batches):\n",
        "        val_loss += s.run(decoder.loss, generate_batch(val_img_embeds,\n",
        "                                                       val_captions_indexed, \n",
        "                                                       batch_size, \n",
        "                                                       MAX_LEN))\n",
        "    val_loss /= n_validation_batches\n",
        "    \n",
        "    print('Epoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    # save weights after finishing epoch\n",
        "    saver.save(s, get_checkpoint_path(epoch))\n",
        "    \n",
        "print(\"Finished!\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Training loss: 3.098571\n",
            "Epoch: 0, train loss: 3.0985707650184633, val loss: 3.067671067714691\n",
            "**************************************************\n",
            "Training loss: 2.758530\n",
            "Epoch: 1, train loss: 2.758529740810394, val loss: 2.9773271584510805\n",
            "**************************************************\n",
            "Training loss: 2.550270\n",
            "Epoch: 2, train loss: 2.5502695982456207, val loss: 2.8875528812408446\n",
            "**************************************************\n",
            "Training loss: 2.414244\n",
            "Epoch: 3, train loss: 2.4142438690662384, val loss: 2.9116009187698366\n",
            "**************************************************\n",
            "Training loss: 2.289532\n",
            "Epoch: 4, train loss: 2.289532083749771, val loss: 2.884495747089386\n",
            "**************************************************\n",
            "Training loss: 2.197430\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0711 01:11:52.454319 139654049408896 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5, train loss: 2.197430156946182, val loss: 2.944124858379364\n",
            "**************************************************\n",
            "Training loss: 2.101491\n",
            "Epoch: 6, train loss: 2.1014910360574723, val loss: 2.963227038383484\n",
            "**************************************************\n",
            "Training loss: 2.019545\n",
            "Epoch: 7, train loss: 2.0195446170568467, val loss: 3.040666756629944\n",
            "**************************************************\n",
            "Training loss: 1.940969\n",
            "Epoch: 8, train loss: 1.9409690611362458, val loss: 3.071781952381134\n",
            "**************************************************\n",
            "Training loss: 1.864024\n",
            "Epoch: 9, train loss: 1.8640236037969589, val loss: 3.104189293384552\n",
            "**************************************************\n",
            "Training loss: 1.790942\n",
            "Epoch: 10, train loss: 1.7909419717788697, val loss: 3.1874444031715394\n",
            "**************************************************\n",
            "Training loss: 1.728954\n",
            "Epoch: 11, train loss: 1.7289542013406753, val loss: 3.262008259296417\n",
            "Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKCNtTB4nLE4",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaUQ9ghjnMeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}